## Date
2025-12-06

## What I Worked On
- Conducted a survey of state-of-the-art Vision-Language Models (VLMs) to identify 3â€“5 candidates for testing augmentation techniques.
- Initially consulted ChatGPT and Gemini to generate a list of 10 potential VLMs, then compared their outputs to shortlist promising options.
- Researched the deployability of these models on NVIDIA Jetson devices to ensure compatibility with my experimental setup.
- Explored the Jetson AI Lab model catalog to identify models optimized for Jetson hardware.
- Discovered Qwen VLM, recommended by both ChatGPT and Gemini, as a potential candidate.
- Studied Qwen models in detail on Hugging Face, including architecture, model variants, and benchmark performance.
- Noted that the Qwen 2.5 7B model is ranked 4th overall and has strong benchmark results on image and video tasks, while the Qwen3 4B model is ranked 1st but is not yet compatible with Jetson JetPack 6.
- Based on compatibility and benchmark performance, finalized the Qwen 2.5 7B model as the primary candidate for my augmentation experiments.

## Issues / Challenges
- Many high-ranking VLMs are currently not deployable on Jetson JetPack 6, limiting model selection.
- Qwen3 4B, despite being top-ranked, cannot be used on my target hardware at this time.
- Needed to balance time investment in model selection since it is not the core focus of my thesis work.

## Progress / Insights
- Gained a clearer understanding of which VLMs are practical for deployment on Jetson devices.
- Identified Qwen 2.5 7B as a viable model combining both strong benchmark performance and hardware compatibility.
- Learned about Qwen model architecture, family variants, and their performance across different tasks.
- Developed a systematic approach for quickly evaluating VLMs for future experiments without excessive time expenditure.
