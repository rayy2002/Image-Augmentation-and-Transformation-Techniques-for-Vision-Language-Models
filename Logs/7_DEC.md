## Date
2025-12-09

## What I Worked On
- Continued exploring the Qwen models after finalizing Qwen 2.5 7B as the first candidate for my thesis experiments.
- Studied the overall architecture of VLMs, focusing on how vision encoders interact with language models.
- Learned about the internal structure of Qwen models, including how they process multimodal inputs and how their components are organized.
- Visited the official Qwen website to explore their documentation and available resources.
- Tested the Qwen 2.5 7B Omni model through their online playground to examine its real-world capabilities.
- Experimented with different input options offered in the playground and evaluated the model’s responses.
- Performed a test using an image of a car with two cranes in the background—one clearly visible and the other less obvious.
- Asked the model, “How many cranes do you see here?” and observed its reasoning:
  - Initially, it detected only one crane.
  - After annotating both cranes manually, the model updated its answer and correctly identified two cranes.
- Treated this as a hands-on exploration to understand the model’s perception abilities and multimodal behavior.

![Screenshot](/images/Screenshot%202025-12-09%20211507.png)


## Issues / Challenges
- The model initially missed one of the cranes in the image due to low visibility.
- Required manual annotation to help the model understand the full context, revealing some limitations in detection accuracy.

## Progress / Insights
- Improved understanding of VLM architecture, especially the role of vision encoders.
- Gained practical insight into how Qwen 2.5 7B Omni handles visual reasoning tasks.
- Learned how annotation can guide the model’s perception and improve response accuracy.
- Developed a clearer sense of how these models behave in real-world multimodal scenarios, preparing for future augmentation experiments.
